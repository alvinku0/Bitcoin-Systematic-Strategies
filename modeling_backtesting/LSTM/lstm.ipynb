{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow scikit-learn numpy scipy scikeras pandas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/full_dataset_feature_engineering_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMESTEPS = 12 \n",
    "N_FEATURES = X_train.shape[2]  # Number of features in the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79973f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"target\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the scalers and feature list\n",
    "with open('./pickles/stat_feature_scaler.pkl', 'rb') as file:\n",
    "    feature_scaler = pickle.load(file)\n",
    "\n",
    "with open('./pickles/stat_target_scaler.pkl', 'rb') as file:\n",
    "    target_scaler = pickle.load(file)\n",
    "\n",
    "with open('./pickles/stat_selected_features.pkl', 'rb') as file:\n",
    "    selected_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_df = df[selected_features].values\n",
    "y_df = df['return_forward'].values  # Replace with your target column name\n",
    "original_indexes = df.index.tolist()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# If your target needs scaling too (for regression problems)\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y_df.reshape(-1, 1))\n",
    "\n",
    "# Define sequence length (time steps to look back)\n",
    "sequence_length = 12  # Adjust based on your specific problem\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "sequence_indexes = []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    X_sequences.append(X_scaled[i:i+sequence_length])\n",
    "    y_sequences.append(y_scaled[i+sequence_length])\n",
    "    sequence_indexes.append(original_indexes[i+sequence_length])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Check the resulting shapes\n",
    "print(f\"X shape: {X_sequences.shape}\")  # Should be (samples, sequence_length, num_features)\n",
    "print(f\"y shape: {y_sequences.shape}\")  # Should be (samples, 1) or (samples,)\n",
    "\n",
    "# Split into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test ,train_idx, test_idx = train_test_split(\n",
    "    X_sequences, y_sequences, sequence_indexes, test_size=0.25062, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Total sequences: {len(sequence_indexes)}\")\n",
    "print(f\"Training sequences: {len(train_idx)}\")\n",
    "print(f\"Testing sequences: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(lstm_units=50, dropout_rate=0.2, recurrent_dropout_rate=0.2,\n",
    "                      learning_rate=0.001, optimizer_name='adam',\n",
    "                      n_timesteps=N_TIMESTEPS, n_features=N_FEATURES):\n",
    "    \"\"\"\n",
    "    Creates and compiles a Keras LSTM model.\n",
    "    Accepts hyperparameters as arguments.\n",
    "    \"\"\"\n",
    "    model = Sequential(name=\"LSTM_Classifier\")\n",
    "    model.add(Input(shape=(n_timesteps, n_features), name=\"Input_Layer\"))\n",
    "    model.add(LSTM(units=lstm_units,\n",
    "                   dropout=dropout_rate,\n",
    "                   recurrent_dropout=recurrent_dropout_rate,\n",
    "                   name=\"LSTM_Layer\"))\n",
    "    model.add(Dropout(dropout_rate, name=\"Post_LSTM_Dropout\")) # Optional extra dropout\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"Output_Layer\")) # Sigmoid for binary classification\n",
    "\n",
    "    # Select optimizer based on name and set learning rate\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Compile model - binary crossentropy for binary classification\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Use scikeras wrapper\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # Old way\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score # Or other relevant metrics\n",
    "from scipy.stats import randint, uniform # For sampling distributions\n",
    "\n",
    "# Optional: Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e60de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__lstm_units': randint(16, 128),          # Number of LSTM units (integer)\n",
    "    'model__dropout_rate': uniform(0.0, 0.5),       # Dropout rate (float)\n",
    "    'model__recurrent_dropout_rate': uniform(0.0, 0.5), # Recurrent dropout rate (float)\n",
    "    'model__learning_rate': uniform(0.0001, 0.01),  # Learning rate (float)\n",
    "    'batch_size': [32, 64, 128],                   # Batch size for training (categorical/integer)\n",
    "    'epochs': [20, 50, 80]                         # Number of training epochs (categorical/integer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afe0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create KerasClassifier wrapper ---\n",
    "# Pass parameters that are *fixed* during the search but needed by the model function\n",
    "# Use verbose=0 inside the wrapper to avoid excessive Keras logs during search\n",
    "keras_estimator = KerasClassifier(\n",
    "    model=create_lstm_model,\n",
    "    # Pass fixed params needed by create_lstm_model NOT being tuned here\n",
    "    model__n_timesteps=N_TIMESTEPS,\n",
    "    model__n_features=N_FEATURES,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# --- Define Callbacks (Optional but Recommended) ---\n",
    "# Early stopping prevents overfitting and speeds up search if models converge early\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,         # Stop after 10 epochs with no improvement\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value\n",
    "    verbose=0            # Set to 1 to see when stopping occurs\n",
    ")\n",
    "\n",
    "# --- Setup Randomized Search CV ---\n",
    "N_ITER_SEARCH = 15  # How many parameter combinations to try\n",
    "CV_FOLDS = 3       # Number of cross-validation folds\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=keras_estimator,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=N_ITER_SEARCH,                 # Number of parameter settings that are sampled\n",
    "    cv=CV_FOLDS,                          # Cross-validation strategy\n",
    "    scoring='accuracy',                   # Metric to optimize (can use custom scorer)\n",
    "    verbose=2,                            # Higher verbose level shows more info\n",
    "    n_jobs=1,                             # Use 1 job to avoid potential GPU memory issues\n",
    "                                          # Set to -1 to use all CPUs, but be careful with GPUs\n",
    "    random_state=SEED,                    # For reproducible sampling\n",
    "    error_score='raise'                   # Raise errors during model fitting\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Randomized Search...\")\n",
    "\n",
    "# Pass callbacks to the fit method of RandomizedSearchCV\n",
    "# These will be used during the training of each candidate model\n",
    "search_result = random_search.fit(X_train, y_train,\n",
    "                                  callbacks=[early_stopping],\n",
    "                                  validation_split=0.2) # Use a portion of training data for early stopping validation\n",
    "\n",
    "print(\"Randomized Search Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest Score (Accuracy): {search_result.best_score_:.4f}\")\n",
    "print(\"Best Parameters Found:\")\n",
    "for param, value in search_result.best_params_.items():\n",
    "    # Adjust param name for display if it has 'model__' prefix\n",
    "    display_param = param.replace('model__', '')\n",
    "    print(f\"- {display_param}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
