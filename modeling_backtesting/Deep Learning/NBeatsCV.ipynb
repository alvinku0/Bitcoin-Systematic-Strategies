{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e2ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/codespace/.local/lib/python3.12/site-packages (19.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b22f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd786dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/full_dataset_feature_engineering_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92333629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 05:50:33.312786: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-21 05:50:33.317777: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-21 05:50:33.333420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745214633.360181   45066 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745214633.368113   45066 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745214633.388530   45066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745214633.388555   45066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745214633.388557   45066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745214633.388559   45066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-21 05:50:33.395172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "with open('./pickles/hybrid_feature_scaler.pkl', 'rb') as file:\n",
    "    feature_scaler = pickle.load(file)\n",
    "\n",
    "with open('./pickles/hybrid_target_scaler.pkl', 'rb') as file:\n",
    "    target_scaler = pickle.load(file)\n",
    "\n",
    "with open('./pickles/hybrid_selected_features.pkl', 'rb') as file:\n",
    "    selected_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c501e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (35046, 12, 15)\n",
      "y shape: (35046, 1)\n",
      "Total sequences: 35046\n",
      "Training sequences: 26262\n",
      "Testing sequences: 8784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_df = df[selected_features].values\n",
    "y_df = df['return_forward'].values  \n",
    "original_indexes = df.index.tolist()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y_df.reshape(-1, 1))\n",
    "\n",
    "\n",
    "sequence_length = 12  \n",
    "\n",
    "\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "sequence_indexes = []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    X_sequences.append(X_scaled[i:i+sequence_length])\n",
    "    y_sequences.append(y_scaled[i+sequence_length])\n",
    "    sequence_indexes.append(original_indexes[i+sequence_length])\n",
    "\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "\n",
    "print(f\"X shape: {X_sequences.shape}\")  \n",
    "print(f\"y shape: {y_sequences.shape}\")  \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test ,train_idx, test_idx = train_test_split(\n",
    "    X_sequences, y_sequences, sequence_indexes, test_size=0.25062, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Total sequences: {len(sequence_indexes)}\")\n",
    "print(f\"Training sequences: {len(train_idx)}\")\n",
    "print(f\"Testing sequences: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d436a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class NBeatsGenericBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Generic N-BEATS block using fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_timesteps: int,\n",
    "                 output_timesteps: int,\n",
    "                 layer_widths: list, \n",
    "                 theta_dim: int, \n",
    "                 backcast_length_factor: int = 1, \n",
    "                 dropout_rate: float = 0.1,\n",
    "                 activation: str = 'relu',\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_timesteps = input_timesteps\n",
    "        self.output_timesteps = output_timesteps\n",
    "        self.layer_widths = layer_widths\n",
    "        self.theta_dim = theta_dim \n",
    "        self.backcast_length = input_timesteps * backcast_length_factor\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for width in layer_widths:\n",
    "            self.hidden_layers.append(layers.Dense(width, activation=activation))\n",
    "            if dropout_rate > 0:\n",
    "                 self.hidden_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "        self.theta_layer = layers.Dense(theta_dim, activation=activation, name='theta')\n",
    "\n",
    "        \n",
    "        \n",
    "        self.forecast_layer = layers.Dense(output_timesteps, activation='linear', name='forecast')\n",
    "        self.backcast_layer = layers.Dense(self.backcast_length, activation='linear', name='backcast')\n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            \n",
    "            if isinstance(layer, layers.Dropout):\n",
    "                x = layer(x, training=training)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        theta = self.theta_layer(x)\n",
    "\n",
    "        forecast = self.forecast_layer(theta)\n",
    "        backcast = self.backcast_layer(theta)\n",
    "\n",
    "        return forecast, backcast\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'input_timesteps': self.input_timesteps,\n",
    "            'output_timesteps': self.output_timesteps,\n",
    "            'layer_widths': self.layer_widths,\n",
    "            'theta_dim': self.theta_dim,\n",
    "            'backcast_length_factor': self.backcast_length // self.input_timesteps,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_nbeats_model(\n",
    "    input_timesteps: int,\n",
    "    output_timesteps: int,\n",
    "    num_stacks: int,         \n",
    "    theta_dim: int,          \n",
    "    layer_widths: list,      \n",
    "    \n",
    "    num_blocks_per_stack: int = 3, \n",
    "    dropout_rate: float = 0.1,\n",
    "    activation: str = 'relu',\n",
    "    learning_rate: float = 0.001,\n",
    "    loss_function: str = 'mean_squared_error'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a Generic N-BEATS Keras model.\n",
    "\n",
    "    Args:\n",
    "        input_timesteps: Length of the input sequence (lookback window).\n",
    "        output_timesteps: Length of the output sequence (forecast horizon).\n",
    "        num_stacks: Number of stacks in the model (Tuned).\n",
    "        theta_dim: Bottleneck dimension before final projection in blocks (Tuned).\n",
    "        layer_widths: List of hidden layer widths within each block (Tuned).\n",
    "        num_blocks_per_stack: Number of blocks within each stack (Fixed).\n",
    "        dropout_rate: Dropout rate applied within blocks (Fixed).\n",
    "        activation: Activation function for hidden layers (Fixed).\n",
    "        learning_rate: Optimizer learning rate (Fixed).\n",
    "        loss_function: Loss function for compilation (Fixed).\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras Model.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = layers.Input(shape=(input_timesteps,), name='input') \n",
    "\n",
    "    residual = inputs\n",
    "    total_forecast = 0\n",
    "\n",
    "    for stack_id in range(num_stacks):\n",
    "        stack_forecast = 0\n",
    "        for block_id in range(num_blocks_per_stack):\n",
    "            block = NBeatsGenericBlock(\n",
    "                input_timesteps=input_timesteps,\n",
    "                output_timesteps=output_timesteps,\n",
    "                layer_widths=layer_widths, \n",
    "                theta_dim=theta_dim,       \n",
    "                dropout_rate=dropout_rate,\n",
    "                activation=activation,\n",
    "                name=f'stack_{stack_id}_block_{block_id}'\n",
    "            )\n",
    "            forecast, backcast = block(residual)\n",
    "            residual = layers.Subtract(name=f'subtract_{stack_id}_{block_id}')([residual, backcast])\n",
    "            stack_forecast = layers.Add(name=f'add_stack_forecast_{stack_id}_{block_id}')([stack_forecast, forecast])\n",
    "\n",
    "        total_forecast = layers.Add(name=f'add_total_forecast_{stack_id}')([total_forecast, stack_forecast])\n",
    "\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=total_forecast, name='NBEATS_Generic')\n",
    "\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NBEATS Tuning ---\n",
      "Input shape: (Timesteps=12, Features=15)\n",
      "Output shape: (Forecast Steps=1)\n",
      "Tuning Params: num_stacks, theta_dim, layer_widths\n",
      "Fixed Params: LR=0.005, Batch=32, Blocks=2, Horizon=1\n",
      "Search: 5 iterations, 2 folds\n",
      "------------------------------\n",
      "Fitting RandomizedSearchCV...\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "[Placeholder Model] Creating with Stacks=3, Theta=32, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:09:53.753361: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-04-21 06:09:53.911041: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-04-21 06:11:16.054463: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=3, model__theta_dim=32; total time= 1.4min\n",
      "[Placeholder Model] Creating with Stacks=3, Theta=32, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:12:34.672554: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=3, model__theta_dim=32; total time= 1.3min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=64, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:13:55.646867: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=64; total time= 1.4min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=64, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:15:15.625954: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=64; total time= 1.3min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=32, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:16:34.273235: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=32; total time= 1.3min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=32, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:17:56.595044: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=32; total time= 1.4min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=32, Width=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:19:22.662653: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=128, model__num_stacks=2, model__theta_dim=32; total time= 1.4min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=32, Width=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:20:39.965282: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=128, model__num_stacks=2, model__theta_dim=32; total time= 1.3min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=64, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:22:03.388313: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=64; total time= 1.4min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=64, Width=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 06:23:12.688023: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=32, model__layer_widths=64, model__num_stacks=2, model__theta_dim=64; total time= 1.2min\n",
      "[Placeholder Model] Creating with Stacks=2, Theta=64, Width=64\n",
      "\n",
      "Randomized Search Finished.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_PredictScorer' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# --- Process Results ---\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_results:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Score (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msearch_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer_\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_results.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Parameters Found:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Filter parameters to show only the ones tuned\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: '_PredictScorer' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Concatenate \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_nbeats_model(\n",
    "    \n",
    "    num_stacks: int = 3,\n",
    "    theta_dim: int = 128,      \n",
    "    layer_widths: int = 256,   \n",
    "    \n",
    "    num_blocks_per_stack: int = 3,\n",
    "    share_weights_in_stack: bool = False,\n",
    "    stack_types: list = None, \n",
    "    \n",
    "    n_timesteps: int = 50,     \n",
    "    n_forecast_steps: int = 10,\n",
    "    n_features: int = 1,       \n",
    "    learning_rate: float = 0.001,\n",
    "    loss_function: str = 'mean_squared_error',\n",
    "    optimizer_name: str = 'adam'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    *** PLACEHOLDER *** Creates a Keras model simulating NBEATS structure.\n",
    "    Replace this with your actual NBEATS implementation compatible with Keras.\n",
    "    It must accept num_stacks, theta_dim, layer_widths, and other params.\n",
    "    \"\"\"\n",
    "    print(f\"[Placeholder Model] Creating with Stacks={num_stacks}, Theta={theta_dim}, Width={layer_widths}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    inputs = keras.Input(shape=(n_timesteps, n_features), name=\"Input\")\n",
    "    \n",
    "    x = layers.Flatten()(inputs)\n",
    "    \n",
    "    x = layers.Dense(layer_widths, activation='relu', name=f\"Hidden_Width_{layer_widths}\")(x)\n",
    "    x = layers.Dense(layer_widths // 2, activation='relu')(x) \n",
    "    \n",
    "    x = layers.Dense(theta_dim, activation='relu', name=f\"Bottleneck_Theta_{theta_dim}\")(x)\n",
    "    \n",
    "    outputs = layers.Dense(n_forecast_steps, activation='linear', name=\"Output\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=f\"Placeholder_NBEATS_S{num_stacks}\")\n",
    "\n",
    "    \n",
    "    if optimizer_name.lower() == 'adam': optimizer = Adam(learning_rate=learning_rate)\n",
    "    else: optimizer = Adam(learning_rate=learning_rate) \n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def tune_nbeats_hyperparameters(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray, \n",
    "    n_iter: int = 15,\n",
    "    cv: int = 3,\n",
    "    \n",
    "    num_stacks_range: tuple = (2, 6),      \n",
    "    theta_dim_options: list = [64, 128, 256], \n",
    "    layer_width_options: list = [128, 256, 512], \n",
    "    \n",
    "    n_forecast_steps: int = 10,           \n",
    "    fixed_num_blocks_per_stack: int = 3,\n",
    "    fixed_share_weights: bool = False,\n",
    "    fixed_stack_types: list = None,       \n",
    "    \n",
    "    fixed_learning_rate: float = 0.001,\n",
    "    fixed_batch_size: int = 64,\n",
    "    fixed_epochs: int = 100,\n",
    "    early_stopping_patience: int = 10,\n",
    "    scoring_metric: str = 'neg_mean_squared_error', \n",
    "    random_state: int = 42\n",
    "    ) -> RandomizedSearchCV:\n",
    "    \"\"\"\n",
    "    Performs RandomizedSearchCV for an NBEATS regression model, focusing on\n",
    "    num_stacks, theta_dim, and layer_widths.\n",
    "\n",
    "    Requires a 'create_nbeats_model' function implementing the NBEATS architecture.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features (samples, n_timesteps, n_features).\n",
    "        y_train: Training target values (samples, n_forecast_steps).\n",
    "        n_iter: Number of parameter settings sampled by RandomizedSearchCV.\n",
    "        cv: Number of cross-validation folds.\n",
    "        num_stacks_range: Tuple (min, max) for sampling number of stacks.\n",
    "        theta_dim_options: List of choices for the theta dimension.\n",
    "        layer_width_options: List of choices for FC layer widths within blocks.\n",
    "        n_forecast_steps: The number of steps the model should predict.\n",
    "        fixed_num_blocks_per_stack: Fixed number of blocks per stack.\n",
    "        fixed_share_weights: Fixed weight sharing setting.\n",
    "        fixed_stack_types: Fixed stack types (if applicable).\n",
    "        fixed_learning_rate: Learning rate to use.\n",
    "        fixed_batch_size: Batch size to use.\n",
    "        fixed_epochs: Max epochs for training each model.\n",
    "        early_stopping_patience: Patience for EarlyStopping.\n",
    "        scoring_metric: Scikit-learn scorer name.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Fitted RandomizedSearchCV object containing the results, or None on error.\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    if X_train.ndim != 3:\n",
    "        raise ValueError(\"X_train must be 3-dimensional (samples, timesteps, features)\")\n",
    "    if y_train.ndim != 2 or y_train.shape[1] != n_forecast_steps:\n",
    "         raise ValueError(f\"y_train must be 2-dimensional (samples, n_forecast_steps={n_forecast_steps})\")\n",
    "\n",
    "    n_timesteps = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    print(f\"--- Starting NBEATS Tuning ---\")\n",
    "    print(f\"Input shape: (Timesteps={n_timesteps}, Features={n_features})\")\n",
    "    print(f\"Output shape: (Forecast Steps={n_forecast_steps})\")\n",
    "    print(f\"Tuning Params: num_stacks, theta_dim, layer_widths\")\n",
    "    print(f\"Fixed Params: LR={fixed_learning_rate}, Batch={fixed_batch_size}, \"\n",
    "          f\"Blocks={fixed_num_blocks_per_stack}, Horizon={n_forecast_steps}\")\n",
    "    print(f\"Search: {n_iter} iterations, {cv} folds\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    \n",
    "    param_distributions = {\n",
    "        \n",
    "        'model__num_stacks': randint(num_stacks_range[0], num_stacks_range[1]),\n",
    "        'model__theta_dim': theta_dim_options,\n",
    "        'model__layer_widths': layer_width_options,\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        'batch_size': [fixed_batch_size] \n",
    "    }\n",
    "\n",
    "    \n",
    "    keras_regressor = KerasRegressor(\n",
    "        model=create_nbeats_model, \n",
    "        \n",
    "        model__n_timesteps=n_timesteps,\n",
    "        model__n_forecast_steps=n_forecast_steps,\n",
    "        model__n_features=n_features,\n",
    "        model__num_blocks_per_stack=fixed_num_blocks_per_stack,\n",
    "        model__share_weights_in_stack=fixed_share_weights,\n",
    "        model__stack_types=fixed_stack_types, \n",
    "        model__learning_rate=fixed_learning_rate,\n",
    "        model__loss_function='mean_squared_error', \n",
    "        model__optimizer_name='adam',             \n",
    "        \n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=['mae', 'mse'],\n",
    "        \n",
    "        epochs=fixed_epochs,\n",
    "        random_state=random_state,\n",
    "        verbose=0 \n",
    "    )\n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    random_search_nbeats = RandomizedSearchCV(\n",
    "        estimator=keras_regressor,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring_metric,\n",
    "        verbose=2, \n",
    "        n_jobs=1, \n",
    "        random_state=random_state,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    \n",
    "    try:\n",
    "        print(\"Fitting RandomizedSearchCV...\")\n",
    "        \n",
    "        search_result = random_search_nbeats.fit(\n",
    "            X_train, y_train,\n",
    "            callbacks=[early_stopping],\n",
    "            validation_split=0.2 \n",
    "        )\n",
    "        print(\"\\nRandomized Search Finished.\")\n",
    "        return search_result\n",
    "\n",
    "    except Exception as e: \n",
    "        print(f\"\\nAn error occurred during Randomized Search for NBEATS.\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "search_results = tune_nbeats_hyperparameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    n_iter=5, \n",
    "    cv=2,      \n",
    "    num_stacks_range=(2, 4),         \n",
    "    theta_dim_options=[32, 64],     \n",
    "    layer_width_options=[64, 128],  \n",
    "    n_forecast_steps=y_train.shape[1], \n",
    "    fixed_num_blocks_per_stack=2,    \n",
    "    fixed_learning_rate=0.005,\n",
    "    fixed_batch_size=32,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "if search_results:\n",
    "    print(f\"\\nBest Score ({search_results.scorer_.__name__}): {search_results.best_score_:.4f}\")\n",
    "    print(\"Best Parameters Found:\")\n",
    "    \n",
    "    tuned_params = ['num_stacks', 'theta_dim', 'layer_widths', 'batch_size']\n",
    "    best_params_display = {}\n",
    "    for k, v in search_results.best_params_.items():\n",
    "        clean_key = k.replace('model__', '')\n",
    "        if clean_key in tuned_params or k == 'batch_size':\n",
    "                best_params_display[clean_key] = v\n",
    "    print(best_params_display)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"\\nNBEATS Hyperparameter tuning failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=2, error_score='raise',\n",
      "                   estimator=KerasRegressor(epochs=100, loss='mean_squared_error', metrics=['mae', 'mse'], model=<function create_nbeats_model at 0x7036feb38e00>, model__learning_rate=0.005, model__loss_function='mean_squared_error', model__n_features=15, model__n_forecast_steps=1, model__n_timesteps=12, model__num_blocks_per_stack=2, mo...ack=False, model__stack_types=None, optimizer='adam', random_state=42, verbose=0),\n",
      "                   n_iter=5, n_jobs=1,\n",
      "                   param_distributions={'batch_size': [32],\n",
      "                                        'model__layer_widths': [64, 128],\n",
      "                                        'model__num_stacks': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7036ff2f67e0>,\n",
      "                                        'model__theta_dim': [32, 64]},\n",
      "                   random_state=42, scoring='neg_mean_squared_error',\n",
      "                   verbose=2)\n"
     ]
    }
   ],
   "source": [
    "print(search_results.param_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1390d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'model__layer_widths': 64, 'model__num_stacks': 2, 'model__theta_dim': 64}\n"
     ]
    }
   ],
   "source": [
    "print(search_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8a9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7036ff2f67e0>\n"
     ]
    }
   ],
   "source": [
    "print(search_results.param_distributions[\"model__num_stacks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f2d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Data Shapes: X=(26262, 12, 15), y=(26262, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X_train must be 2-dimensional (samples, input_timesteps) for this N-BEATS setup. Found 3 dims.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# --- Call the tuning function ---\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Reduce n_iter for quick demo\u001b[39;00m\n\u001b[32m    194\u001b[39m SEED = \u001b[32m42\u001b[39m \u001b[38;5;66;03m# Define seed if not globally defined\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m search_results_nbeats = \u001b[43mtune_nbeats_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced for speed in demo\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Reduced for speed in demo\u001b[39;49;00m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_stacks_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Example range (2 or 3 stacks)\u001b[39;49;00m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta_dim_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Example options\u001b[39;49;00m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_width_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Example options for hidden layer width\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_learning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.002\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# --- Process Results ---\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_results_nbeats:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtune_nbeats_hyperparameters\u001b[39m\u001b[34m(X_train, y_train, output_timesteps, n_iter, cv, num_stacks_range, theta_dim_options, layer_width_options, fixed_num_blocks_per_stack, fixed_learning_rate, fixed_batch_size, fixed_dropout_rate, fixed_activation, fixed_epochs, early_stopping_patience, scoring_metric, random_state)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Derive input shape from data\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_train.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX_train must be 2-dimensional (samples, input_timesteps) for this N-BEATS setup. Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dims.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m input_timesteps = X_train.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Verify y_train shape\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: X_train must be 2-dimensional (samples, input_timesteps) for this N-BEATS setup. Found 3 dims."
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform \n",
    "\n",
    "def tune_nbeats_hyperparameters(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray, \n",
    "    output_timesteps: int,\n",
    "    n_iter: int = 5,\n",
    "    cv: int = 2,\n",
    "    \n",
    "    num_stacks_range: tuple = (2, 6),      \n",
    "    theta_dim_options: list = [64, 128, 256], \n",
    "    layer_width_options: list = [128, 256, 512], \n",
    "    \n",
    "    fixed_num_blocks_per_stack: int = 3,\n",
    "    fixed_learning_rate: list = [0.001,0.002,0.003], \n",
    "    fixed_batch_size: int = 128,\n",
    "    fixed_dropout_rate: float = 0.1,\n",
    "    fixed_activation: str = 'relu',\n",
    "    fixed_epochs: int = 100,\n",
    "    early_stopping_patience: int = 10,\n",
    "    scoring_metric: str = 'neg_mean_squared_error', \n",
    "    random_state: int = 42\n",
    "    ) -> RandomizedSearchCV:\n",
    "    \"\"\"\n",
    "    Performs RandomizedSearchCV for a Generic N-BEATS regression model, focusing on\n",
    "    num_stacks, theta_dim (as bottleneck), and layer_widths.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features (samples, input_timesteps). N-BEATS often takes flat input.\n",
    "        y_train: Training target values (samples, output_timesteps).\n",
    "        output_timesteps: The forecast horizon length.\n",
    "        n_iter: Number of parameter settings sampled by RandomizedSearchCV.\n",
    "        cv: Number of cross-validation folds.\n",
    "        num_stacks_range: Tuple (min, max) for randint sampling of stack count.\n",
    "        theta_dim_options: List of choices for the block's bottleneck dimension.\n",
    "        layer_width_options: List of choices for the hidden layer width within blocks.\n",
    "        fixed_num_blocks_per_stack: Fixed number of blocks per stack.\n",
    "        fixed_learning_rate: Learning rate to use (fixed).\n",
    "        fixed_batch_size: Batch size to use (fixed).\n",
    "        fixed_dropout_rate: Dropout rate to use.\n",
    "        fixed_activation: Activation function ('relu' or 'gelu').\n",
    "        fixed_epochs: Max epochs for training each model.\n",
    "        early_stopping_patience: Patience for EarlyStopping callback.\n",
    "        scoring_metric: Scikit-learn scorer name for evaluation.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Fitted RandomizedSearchCV object containing the results.\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    if X_train.ndim != 2:\n",
    "        raise ValueError(f\"X_train must be 2-dimensional (samples, input_timesteps) for this N-BEATS setup. Found {X_train.ndim} dims.\")\n",
    "    input_timesteps = X_train.shape[1]\n",
    "\n",
    "    \n",
    "    if y_train.ndim != 2 or y_train.shape[1] != output_timesteps:\n",
    "         raise ValueError(f\"y_train must be 2-dimensional (samples, output_timesteps={output_timesteps}). Found shape {y_train.shape}.\")\n",
    "\n",
    "\n",
    "    print(f\"--- Starting N-BEATS Tuning ---\")\n",
    "    print(f\"Input shape: (Timesteps={input_timesteps})\")\n",
    "    print(f\"Output shape: (Timesteps={output_timesteps})\")\n",
    "    print(f\"Tuning Params: num_stacks, theta_dim, layer_widths (single hidden layer width)\")\n",
    "    print(f\"Fixed Params: LR={fixed_learning_rate}, Batch={fixed_batch_size}, \"\n",
    "          f\"Dropout={fixed_dropout_rate}, Blocks/Stack={fixed_num_blocks_per_stack}, Act={fixed_activation}\")\n",
    "    print(f\"Search: {n_iter} iterations, {cv} folds\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    \n",
    "    param_distributions = {\n",
    "        \n",
    "        'model__num_stacks': randint(num_stacks_range[0], num_stacks_range[1]),\n",
    "        'model__theta_dim': theta_dim_options,\n",
    "        \n",
    "        \n",
    "        \n",
    "        'layer_width_single': layer_width_options,\n",
    "\n",
    "        \n",
    "        'batch_size': [fixed_batch_size] \n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    keras_regressor = KerasRegressor(\n",
    "        model=create_nbeats_model,\n",
    "        \n",
    "        model__input_timesteps=input_timesteps,\n",
    "        model__output_timesteps=output_timesteps,\n",
    "        model__num_blocks_per_stack=fixed_num_blocks_per_stack,\n",
    "        model__dropout_rate=fixed_dropout_rate,\n",
    "        model__activation=fixed_activation,\n",
    "        model__learning_rate=fixed_learning_rate,\n",
    "        model__loss_function='mean_squared_error',\n",
    "        \n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=['mae', 'mse'],\n",
    "        \n",
    "        epochs=fixed_epochs,\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        random_state=random_state,\n",
    "        verbose=0 \n",
    "    )\n",
    "\n",
    "    \n",
    "    def model_wrapper_nbeats(**kwargs):\n",
    "        \n",
    "        single_width = kwargs.pop('layer_width_single', 256) \n",
    "        \n",
    "        kwargs['layer_widths'] = [single_width, single_width] \n",
    "        \n",
    "        return create_nbeats_model(**kwargs)\n",
    "\n",
    "    \n",
    "    keras_regressor.set_params(model=model_wrapper_nbeats)\n",
    "\n",
    "    \n",
    "    param_distributions = {\n",
    "        'model__num_stacks': randint(num_stacks_range[0], num_stacks_range[1]),\n",
    "        'model__theta_dim': theta_dim_options,\n",
    "        'model__layer_width_single': layer_width_options, \n",
    "        'batch_size': [fixed_batch_size]\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    random_search_nbeats = RandomizedSearchCV(\n",
    "        estimator=keras_regressor,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring_metric,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        random_state=random_state,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    \n",
    "    try:\n",
    "        print(\"Fitting RandomizedSearchCV...\")\n",
    "        \n",
    "        search_result = random_search_nbeats.fit(\n",
    "            X_train, y_train, \n",
    "            callbacks=[early_stopping],\n",
    "            validation_split=0.2 \n",
    "        )\n",
    "        print(\"\\nRandomized Search Finished.\")\n",
    "        return search_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during Randomized Search for N-BEATS.\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() \n",
    "        return None \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Demo Data Shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42 \n",
    "search_results_nbeats = tune_nbeats_hyperparameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    output_timesteps=12,\n",
    "    n_iter=5, \n",
    "    cv=2,     \n",
    "    num_stacks_range=(2, 4),         \n",
    "    theta_dim_options=[64, 128],     \n",
    "    layer_width_options=[128, 256],  \n",
    "    fixed_learning_rate=0.002,\n",
    "    fixed_batch_size=64,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "if search_results_nbeats:\n",
    "    print(f\"\\nBest Score ({search_results_nbeats.scorer_.__name__}): {search_results_nbeats.best_score_:.4f}\")\n",
    "    print(\"Best Parameters Found:\")\n",
    "    best_params_display = {k.replace('model__', ''): v for k, v in search_results_nbeats.best_params_.items()}\n",
    "    print(best_params_display)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"\\nN-BEATS Hyperparameter tuning failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
